action_selector: "socg"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000
graph_epsilon: 0.00

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
runner: "cg_episode"
mac: "socg_mac"
learner: "socg_learner"
agent: "rnn_cg"
double_q: True
double_q_on_graph: True
communicate: False
asym_alpha: 1.0
construction: 'tree'

comm_rnn_hidden_dim: 32
single_q_hidden_dim: []
pairwise_q_hidden_dim: [64]
individual_q: True

name: "socg"

msg_anytime: True             
msg_normalized: True 